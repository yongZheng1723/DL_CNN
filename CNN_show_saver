import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data
mnist=input_data.read_data_sets("D://anaconda//python//fashion_data",one_hot=True)

batch_size = 256
n_batch = mnist.train.num_examples // batch_size

def weight_variable(shape):
    return tf.Variable(tf.truncated_normal(shape,stddev=0.1))

def bias_vairable(shape):
    return tf.Variable(tf.constant(0.1, shape=shape))

def conv2d(x,W):
    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')

def max_pool_2x2(x):
    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')

x = tf.placeholder(tf.float32,[None,784])
y = tf.placeholder(tf.float32,[None,10])
keep_prob = tf.placeholder(tf.float32)

x_image = tf.reshape(x,[-1,28,28,1])

W_conv1 = weight_variable([5,5,1,16]) # 5*5的采样窗口，16个卷积核从1个平面抽取特征
b_conv1 = bias_vairable([16]) #每个卷积核一个偏置值

# 28*28*1 的图片卷积之后变为28*28*16
h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
# 池化之后变为 14*14*16
h_pool1 = max_pool_2x2(h_conv1)

# 第二次卷积之后变为 14*14*35
W_conv2 = weight_variable([5,5,16,35])
b_conv2 = bias_vairable([35])
h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)
# 第二次池化之后变为 7*7*35
h_pool2 = max_pool_2x2(h_conv2)


# 第一个全连接层
W_fc1 = weight_variable([7*7*35,1024])
b_fc1 = bias_vairable([1024])
# 7*7*35的图像变成1维向量
h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*35])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

# 第二个全连接层
W_fc2 = weight_variable([1024,10])
b_fc2 = bias_vairable([10])
logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2
prediction = tf.nn.sigmoid(logits)

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))
train_step = tf.train.AdamOptimizer(0.0012).minimize(loss)

prediction_2 = tf.nn.softmax(prediction)
correct_prediction = (tf.equal(tf.argmax(prediction_2,1), tf.argmax(y,1)))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

#模型的保存
saver=tf.train.Saver(max_to_keep=3)
# 假如需要保存y，以便在预测时使用
tf.add_to_collection("pred", prediction_2)
tf.add_to_collection('acc', accuracy)
tf.add_to_collection('cost', loss)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    Cost=[]
    Acc=[]
    max_acc=0
    count=0
    for epoch in range(100):
        for batch in range(n_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            sess.run(train_step, feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.5})
        cost=sess.run(loss,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:0.75})
        acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0})
        Cost.append(cost)
        Acc.append(acc)

        if(acc>max_acc):
            max_acc=acc
            saver.save(sess,"D://anaconda//python//CNN1.ckpt")
        print("Iter: " + str(epoch) + ",Loss:"+str(cost)+", acc: " + str(acc))
        count+=1
        print("\r当前进度：{:.2f}%".format(count),end="")
    print('traning finished')    
        #代价函数曲线
    fig1,ax1 = plt.subplots(figsize=(10,7))
    ax1.plot(Cost)
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Cost')
    plt.title('Cross Loss')
    plt.grid()
    plt.show()
    # 准确率曲线
    fig7,ax7 = plt.subplots(figsize=(10,7))
    ax7.plot(Acc)
    ax7.set_xlabel('Epochs')
    ax7.set_ylabel('Accuracy Rate')
    plt.title('Train Accuracy Rate')
    plt.grid()
    plt.show()
    #**************各个层特征可视化***********************
    #imput image
    fig2,ax2=plt.subplots(figsize=(2,2))
    ax2.imshow(np.reshape(mnist.train.images[11],(28,28)))
    plt.show()
    #第一层的卷积输出特征图
    input_image=mnist.train.images[11:12]
    conv1_16=sess.run(h_conv1,feed_dict={x:input_image})#【1,28,28,16】
    conv1_transpose=sess.run(tf.transpose(conv1_16,[3,0,1,2]))
    fig3,ax3=plt.subplots(nrows=1,ncols=16,figsize=(16,1))
    for i in range(16):
        ax3[i].imshow(conv1_transpose[i][0])
    plt.title('Conv 16*28*28')
    plt.show()
    #第一层池化后的特征图
    pool1_16=sess.run(h_pool1,feed_dict={x:input_image})#[1,14,14,16]
    pool1_transpose=sess.run(tf.transpose(pool1_16,[3,0,1,2]))
    fig4,ax4=plt.subplots(nrows=1,ncols=16,figsize=(16,1))
    for i in range(16):
        ax4[i].imshow(pool1_transpose[i][0])
    plt.title('Pool 16*14*14')
    plt.show()
    #第二层的卷积输出特征图
    conv2_35=sess.run(h_conv2,feed_dict={x:input_image})#【1,14,14,35】
    conv2_transpose=sess.run(tf.transpose(conv2_35,[3,0,1,2]))
    fig5,ax5=plt.subplots(nrows=1,ncols=35,figsize=(35,1))
    for i in range(35):
        ax5[i].imshow(conv2_transpose[i][0])
    plt.title('Conv2 35*14*14')
    plt.show()
    #第二层池化后的特征图
    pool2_35=sess.run(h_pool2,feed_dict={x:input_image})#[1,7,7,35]
    pool2_transpose=sess.run(tf.transpose(pool2_35,[3,0,1,2]))
    fig6,ax6=plt.subplots(nrows=1,ncols=35,figsize=(35,1))
    for i in range(35):
        ax6[i].imshow(pool2_transpose[i][0])
    plt.title('Pool2 35*7*7')
    plt.show()
   # saver.save(sess,"D://anaconda//python//model_cnn.ckpt")
